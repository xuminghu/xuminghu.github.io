
<!DOCTYPE html>
<html>
<head>
    <title>Xuming Hu's Personal Website</title>
	<meta name="keywords" content="" />
	<meta name="description" content="" />

	<meta charset="utf-8">
	<meta name="viewport" content="initial-scale=1">
    
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" href="css/bootstrap.min.css">
	<!-- <script src="https://kit.fontawesome.com/f777b7e930.js" crossorigin="anonymous"></script> -->
	<link href="https://kit-pro.fontawesome.com/releases/v6.1.1/css/pro.min.css" rel="stylesheet">
	<!-- <link rel="stylesheet" href="css/font-awesome.min.css"> -->
	<link rel="stylesheet" href="css/templatemo_misc.css">
	<link rel="stylesheet" href="./css/templatemo_style.css">
	<!-- Tab -->
	<link href="https://fonts.googleapis.com/css?family=Work+Sans:400,600,700&display=swap" rel="stylesheet">
</head>
<style>
.outline-detail{
    height: 350px
}
</style>
<body>

	<!-- This one in here is responsive menu for tablet and mobiles -->
    <div class="responsive-navigation visible-sm visible-xs">
        <a href="#" class="menu-toggle-btn">
            <i class="fa fa-bars fa-2x"></i>
        </a>
        <div class="navigation responsive-menu">
            <ul>
                <li class="home"><a href="#templatemo">Home</a></li>
	            <li class="about"><a href="#about">About Me</a></li>
				<li class="news"><a href="#news">News</a></li>
				<li class="research"><a href="#research">Research</a></li>
	            <li class="services"><a href="#services">Services</a></li>
	            <li class="portfolio"><a href="#portfolio">Portfolio</a></li>
				<li class="student"><a href="#student">To Prospective Students</a></li>
				<!-- <li class="ack"><a href="#ack">Acknowledgement</a></li> -->
	            <li class="contact"><a href="#contact">Contact</a></li>
            </ul> <!-- /.main_menu -->
        </div> <!-- /.responsive_menu -->
    </div> <!-- /responsive_navigation -->

	<div id="main-sidebar" class="hidden-xs hidden-sm">
		<div class="logo">
			<a href="#"><h1>Xuming Hu</h1></a>
			<span>Personal Website</span>
		</div> <!-- /.logo -->

		<div class="navigation">
	        <ul class="main-menu">
	            <li class="home"><a href="#templatemo">Home</a></li>
	            <li class="about"><a href="#about">About Me</a></li>
				<li class="news"><a href="#news">News</a></li>
				<li class="research"><a href="#research">Research</a></li>
	            <li class="services"><a href="#services">Services</a></li>
	            <li class="portfolio"><a href="#portfolio">selected publications</a></li>
				<li class="student"><a href="#student">To Prospective Students</a></li>
				<!-- <li class="ack"><a href="#ack">Acknowledgement</a></li> -->
	            <li class="contact"><a href="#contact">Contact</a></li>
	        </ul>
		</div> <!-- /.navigation -->

	</div> <!-- /#main-sidebar -->

	<div id="main-content">

		<div id="templatemo">
			<div class="main-slider">
				<div class="flexslider">
					<ul class="slides">
                    
						<li>
							<div class="slider-caption">
								<h2>Xuming Hu</h2>
								<p>I'm an assistant professor of the AI Thrust at <strong><a href="https://www.hkust-gz.edu.cn/" target="_blank">The Hong Kong University of Science and Technology (Guangzhou).</a></strong></p>
                                <p>I am also being affiliated with the Department of Computer Science and Engineering at The Hong Kong University of Science and Technology (Clear Water Bay).</p>
							</div>
							<img src="images/slide1.jpg" alt="Slide 1">
						</li>
                        
						<li>
							<div class="slider-caption">
								<h2>Xuming Hu</h2>
								<p>My research interests lie in <strong>the fields of Natural Language Processing and Large Language Models.</strong></p> 
								<a href="#" class="largeButton homeBgColor">Details</a>
							</div>
							<img src="images/slide2.jpg" alt="Slide 2">
						</li>
                        
                        <!-- <li>
							<div class="slider-caption">
								<h2>Xuming Hu</h2>
								<p>I'm excited to be a founding member of <strong><a href="https://kumo.ai/" target="_blank">Kumo.ai</a></strong>! </p>
								<a href="Kumo.ai" class="largeButton homeBgColor">More About Kumo</a>
							</div>
							<img src="images/slide3.jpg" alt="Slide 3">
						</li> -->
                        
					</ul>
				</div>
			</div>
			<div class="container-fluid">
				<div class="row">
					<div class="col-md-12">
						<div class="welcome-text">
							<h2>Welcome to my personal website</h2>
							<p>
								Our lab is hiring motivated and capable Ph.D. and MPhil students interested in Natural Language Processing.</p>
						</div>
					</div>
				</div>
			</div>
		</div> <!-- /#sTop -->

		<div class="container-fluid">

			<div id="about" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>About Me</h2>
						</div>
					</div>
				</div>
				<!-- <div class="row">
					<div class="col-md-4">
						<div class="member-item">
							<div class="member-thumb">
								<img src="images/team1.jpg" alt="girl 1">
								<div class="overlay">
									<ul class="social-member">
										<li><a href="#" class="fa fa-facebook"></a></li>
										<li><a href="#" class="fa fa-twitter"></a></li>
										<li><a href="#" class="fa fa-linkedin"></a></li>
									</ul>
								</div>
							</div>
							<div class="member-content">
								<h4>Tanna Dona</h4>
								<p>Marketing Director</p>
							</div>
						</div>
					</div>  -->
					<!-- /.col-md-4 -->
					<!-- <div class="col-md-4">
						<div class="member-item">
							<div class="member-thumb">
								<img src="images/team2.jpg" alt="girl 2">
								<div class="overlay">
									<ul class="social-member">
										<li><a href="#" class="fa fa-facebook"></a></li>
										<li><a href="#" class="fa fa-twitter"></a></li>
										<li><a href="#" class="fa fa-linkedin"></a></li>
									</ul>
								</div>
							</div>
							<div class="member-content">
								<h4>Candy Ball</h4>
								<p>Creative Executive</p>
							</div>
						</div>
					</div>  -->
					<!-- /.col-md-4 -->
					<!-- <div class="col-md-4">
						<div class="member-item">
							<div class="member-thumb">
								<img src="images/team3.jpg" alt="girl 3">
								<div class="overlay">
									<ul class="social-member">
										<li><a href="#" class="fa fa-facebook"></a></li>
										<li><a href="#" class="fa fa-twitter"></a></li>
										<li><a href="#" class="fa fa-linkedin"></a></li>
									</ul>
								</div>
							</div>
							<div class="member-content">
								<h4>Tawana Cherry</h4>
								<p>Company President</p>
							</div>
						</div>
					</div>  -->
					<!-- /.col-md-4 -->
				<!-- </div>  -->
				<!-- /.row -->
				<div class="row">
					<div class="col-md-8">
						<h3>Introduction</h3>
                        Hi! My name is Xuming Hu (胡旭明). I’m an Assistant Professor of the AI Thrust at <strong><a href="https://www.hkust-gz.edu.cn/" target="_blank">The Hong Kong University of Science and Technology (Guangzhou)</a></strong>. I am also being affiliated with the Department of Computer Science and Engineering at The Hong Kong University of Science and Technology (Clear Water Bay). <br>

					</div><br/><br/>
					<div class="col-md-8">
						<h3>Education</h3>
                        I received my Ph.D. at the <strong><a href="https://www.thss.tsinghua.edu.cn/" target="_blank">School of Software</a></strong> in <strong><a href="https://www.tsinghua.edu.cn/" target="_blank">Tsinghua University</a></strong>, where I am advised by Prof. <strong><a href="https://www.cs.uic.edu/PSYu/" target="_blank">Philip S. Yu</a></strong> (ACM/IEEE Fellow). I was a Visiting Scholar at <strong><a href="https://misc-lab.cse.cuhk.edu.hk/people/" target="_blank">CUHK MISC Lab</a></strong> from May 2022 to August 2023 under the supervision of Prof. <strong><a href="https://www.cse.cuhk.edu.hk/irwin.king/home" target="_blank">Irwin King</a></strong> (IEEE Fellow).
						<!-- I did research in geometric algorithms, computational topology, numerical analysis and signal processing. -->
				    </div>
					<div class="col-md-8">
						<h3>Prospective Students</h3>
						We sincerely welcome interested students! <strong>Please refer to this <a href="#student">section</a> for more detail.</strong><br>
						<strong style="color:blue">[Updated on June 30, 2024]</strong> Thanks for your interest in joining our group! We have <strong>3-4 open positions</strong> for Ph.D. students in 2025 Spring and Fall in AI Thrust. For MPhil students, we have <strong>4-5 open positions</strong> in 2024 Fall in AI Thrust, please contact me after passing the interview with the school's Red Bird MPhil committee.
				    </div>
					<div class="member-item">
						<div class="member-thumb">
							<img src="images/Selfie.jpg" alt="selfie" width="50%">
							<!-- <div class="overlay">
								<ul class="social-member">
									<li><a href="https://www.facebook.com/ying.zhitao/" class="fa-brands fa-facebook-square"></a></li>
									<li><a href="https://twitter.com/RexYing0923" class="fa-brands fa-twitter"></a></li>
									<li><a href="https://www.linkedin.com/in/rex-ying-92770148/" class="fa-brands fa-linkedin"></a></li>
								</ul>
							</div> -->
						</div>
						<div class="member-content">
							<h4>Dr. Xuming Hu</h4>
							<p>Assistant professor</p>
						</div>
					</div>
				</div> <!-- /.row -->
			</div> <!-- /#about -->

			<div id="news" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>News</h2>
						</div> <!-- /.section-title -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
				<div class="row">
					<div class="col-md-12">
						<ul class="expand-list">
							<li><strong>2024.06</strong>: Glad to serve as Area Chair for <strong><a href="https://2024.emnlp.org/" target="_blank">EMNLP 2024</a></strong></li>
							<li><strong>2024.06</strong>: I am very honored to be awarded as “<strong><a href="https://info.tsinghua.edu.cn" target="_blank">Outstanding Graduate of Tsinghua University (清华大学优秀毕业生)”</a></strong> and “<strong><a href="https://info.tsinghua.edu.cn" target="_blank">Outstanding Doctoral Dissertation of Tsinghua University (清华大学优秀博士学位论文)”</a></strong></li>
							<li><strong>2024.05</strong>: Three papers on Trustworthy Large Language Models and Large Language Model Agents are accepted by <strong><a href="https://2024.aclweb.org/" target="_blank">ACL 2024</a></strong> (1 Main and 2 Findings)</li>
							<li><strong>2024.04</strong>: One paper on Trustworthy Large Language Models is accepted by <strong><a href="https://sigir-2024.github.io/" target="_blank">SIGIR 2024</a></strong></li>
							<li><strong>2024.02</strong>: One paper on Large Language Model Distillation is accepted by <strong><a href="https://2024.naacl.org/" target="_blank">NAACL 2024</a></strong></li>
							<li><strong>2024.01</strong>: Three papers on Trustworthy Large Language Models are accepted by <strong><a href="https://iclr.cc/" target="_blank">ICLR 2024</a></strong></li>
                            <li><strong>2024.01</strong>: Glad to serve as Area Chair for <strong><a href="https://2024.aclweb.org/" target="_blank">ACL 2024</a></strong></li>
                            <li><strong>2024.01</strong>: I am very honored to be awarded as an “<strong><a href="https://jw.beijing.gov.cn/tzgg/202401/t20240102_3522508.html" target="_blank">Outstanding Graduate of Beijing (北京市优秀毕业生)”</a></strong></li>
                            <li><strong>2023.12</strong>: Glad to serve as Area Chair for <strong><a href="https://2024.naacl.org/" target="_blank">NAACL 2024</a></strong></li>
                            <li><strong>2023.10</strong>: One paper on Document-Level Realtion Extraction is accepted by <strong><a href="https://2023.emnlp.org" target="_blank">EMNLP 2023</a></strong></li>
                            <li><strong>2023.10</strong>: Glad to serve as Area Chair for <strong><a href="https://2024.eacl.org/" target="_blank">EACL 2024</a></strong></li>
                            <li><strong>2023.09</strong>: Glad to serve as Action Editor for <strong><a href="https://aclrollingreview.org/people" target="_blank">ACL Rolling Review</a></strong></li>
                            <li><strong>2023.09</strong>: One paper on Retrieval-Augmented Open Relation Extraction is accepted by  <strong><a href="https://www.computer.org/csdl/journal/tk/5555/01/10255305/1QzyilOWR44" target="_blank">IEEE TKDE</a></strong></li>
							<!-- <li class="collapse-li"><a class="expand emph">Past News</a></li>
								<ul class="expand-list2">
									<li class="collapse-year"> <a>2022</a></li>
									<ul> 
										<li class="posts"></li>
									</ul>
								</ul> -->
							
						</ul>
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->

			</div> <!-- /#news -->

			<div id="research" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>Research Outline</h2>
						</div> <!-- /.section-title -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
				<div class="row">
				<div class="col-md-7">
					<!-- <ul>
					<li>I work on advancing graph neural network (GNN) architectures that improve the 
					expressiveness, scalability and interpretability of GNNs. </li>

					<li>I'm interested in empowering deep learning architectures with effective representation geometry,
					which is essential in modeling data manifolds with different characteristics.
					For example, we explore modeling of hierarchical relations through <a href="https://arxiv.org/abs/1910.12892">hyperbolic embeddings</a>.
				    </li>

					<li>
					I work on real-world applications in physical simulations, 
					chemistry and biology predictions, knowledge graphs, natural languages, recommendations and 
					social networks. 
					What interest me the most are novel ways with which we can 
					identify intrinsic connections within the data and empower machine learning algorithms by leveraging relational information. 
				    </li>    
				    </ul><br /> -->
                    My research interests lie in the fields of natural language processing and deep learning. In particular, I am interested in large language models and focusing on:
                    <ul>
                        <li>Exploring Trustworthy LLMs,</li>
                        <li>Studying Multimodal LLMs,</li>
			<li>Exploring Efficient LLMs,</li>
                        <li>Delving into the "AI for Science" initiative.</li>
                    </ul>
					<br/><br/>
					My list of publications can be found on <a href="https://scholar.google.com/citations?user=dbBKbXoAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>
				</div>
				<!-- <div class="portfolio-item col-md-5">
					<div class="portfolio-thumb">
						<img src="images/word_cloud.png" alt="word_cloud">
					</div>
			    </div> -->
			    </div></br>
				<div class="row">
					<div class="col-md-3">
						<div class="outline-item clearfix">
							<div class="outline-icon">
								<i class="fa-solid fa-circle-nodes fa-2x"></i>
							</div>
							<div class="outline-content">
								<h3>"Hallucination" <br> in LLMs</h3>
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
					<div class="col-md-3">
						<div class="outline-item">
							<div class="outline-icon">
								<i class="fa-solid fa-chart-scatter-bubble fa-2x"></i>
							</div>
							<div class="outline-content">
								<h3>Exploring <br> Trustworthy LLMs</h3>
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
					<div class="col-md-3">
						<div class="outline-item">
							<div class="outline-icon">
								<i class="fa-solid fa-chart-scatter-bubble fa-2x"></i>
							</div>
							<div class="outline-content">
								<h3>Studying <br> Multimodal LLMs</h3>
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
					<div class="col-md-3">
						<div class="outline-item">
							<div class="outline-icon">
								<i class="fa-solid fa-user-gear fa-2x"></i>
							</div>
							<div class="outline-content">
								<h3>AI for <br>Science</h3>
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
				</div> <!-- /.row --><br/>
				<div class="row">
					<div class="col-md-3">
						<div class="outline-item clearfix">
							<div class="outline-detail">
                                Exploring trustworthy LLMs aims to ensure the content generated by these models is sufficiently truthful. We attempt to mitigate the hallucination phenomenon in LLMs through methods such as retrieval augmented and the use of watermarks in LLMs.
								<!-- It is crucial for pretrained foundation models to understand and make use of relational information to empower intelligent reasoning,
								leveraging graph representations. -->
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
					<div class="col-md-3">
						<div class="outline-item">
							<div class="outline-detail">
                                Multimodal LLMs can transcend the boundaries of text and venture into the realm of vision, opening new perspectives for the development of embodied intelligence and sparking imagination about the future application potential of LLMs.
								<!-- The goal is to empower deep learning architectures with effective representation geometry,
					            which is essential in modeling data manifolds with different characteristics. -->
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
					<div class="col-md-3">
						<div class="outline-item">
							<div class="outline-detail">
                               The high cost of training and reasoning of LLMs and the high computing power required have hindered the development of LLMs. How to accelerate the reasoning speed of LLMs, and reduce the parameter scale of LLMs are important research directions.
								<!-- The goal is to empower deep learning architectures with effective representation geometry,
					            which is essential in modeling data manifolds with different characteristics. -->
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
					<div class="col-md-3">
						<div class="outline-item">
							<div class="outline-detail">
                                I explore the real-world applications of LLMs, such as in biology, materials, healthcare, recommendation systems, and social networks, etc., committed to making LLMs better serve society.
								<!-- I work on real-world applications in  
								chemistry, biology, neuroscience, physical simulations, knowledge graphs, natural languages, recommendations and 
								social networks. -->
							</div>
						</div> <!-- /.outline-item -->
					</div> <!-- /.col-md-4 -->
				</div> <!-- /.row -->
		    </div> <!-- /#research outline -->
			
			<div id="services" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>Services</h2>
						</div> <!-- /.section-title -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
				<div class="row ">
					<div class="col-md-12">
                        <ul>
                            <li><strong>Editorial Services</strong> </br>ACL Rolling Review</li>
                            <li><strong>Conference Area Chair</strong> </br>ACL 2024, NAACL 2024, EACL 2024</li>
                            <li><strong>Conference Program Committee Member</strong></br>ACL 2022-2023, EMNLP 2021-2023, NAACL 2022, AAAI 2022-2024, SIGKDD 2023, SIGIR 2023-2024, WWW 2024</li>
                            <li><strong>Journal Reviewer</strong></br>IEEE TKDE, IEEE TNNLS, IEEE/ACM TASLP</li>

                        </ul>
					</div>
				</div>

				<!-- <div class="row our-skills">
					<div class="col-md-12">
					<h3>Teaching</h3>
					I teach two courses: "Deep Learning for Graph-Structured Data" and "Trustworthy Deep Learning" at Yale. 
					<br>
					<h4>Course Websites</h4> 
					<ul>
					    <li><a href="https://graph-and-geometric-learning.github.io/cpsc483-583-website-23fall/#/">Deep Learning for Graph-Structured Data</a></li>
					</ul>
				    </div>
				</div> -->
				
				<!-- <div class="row our-skills">
					<div class="col-md-8">
						<h3>My Past Workshops</h3>
						<ul>
							<li>New Frontiers in Graph Learning (<a href="https://glfrontiers.github.io/" target="_blank">GLFrontiers</a>) at NeurIPS 2022 and NeurIPS 2023</li>
							<li>Deep Learning for Simulation (<a href="https://simdl.github.io/overview/" target="_blank">SimDL</a>) at ICLR 2021</li>
							<li>Stanford Graph Learning Workshop (<a href="https://snap.stanford.edu/graphlearning-workshop/" target="_blank">SGL</a>)</li>
							<li>Graph Representationn Learning and Beyond (<a href="https://grlplus.github.io/" target="_blank">GRL+</a>) at ICML 2020</li>
							<li>Co-organized the 2020 KDD Cup Competition on <a href=" https://www.kdd.org/kdd2020/kdd-cup">Graph AutoML</a>.</li>
						</ul>
					</div>
					<div class="col-md-4">
						<ul class="progess-bars">
							<li>
								<div class="progress">
									<div class="progress-bar" role="progressbar" aria-valuenow="90" aria-valuemin="0" aria-valuemax="100" style="width: 90%;">WEB DESIGN 90%</div>
								</div>
							</li>
							<li>
								<div class="progress">
									<div class="progress-bar" role="progressbar" aria-valuenow="75" aria-valuemin="0" aria-valuemax="100" style="width: 80%;">HTML5 CSS3 80%</div>
								</div>
							</li>
							<li>
								<div class="progress">
									<div class="progress-bar" role="progressbar" aria-valuenow="66" aria-valuemin="0" aria-valuemax="100" style="width: 75%;">WordPress 75%</div>
								</div>
							</li>
							<li>
								<div class="progress">
									<div class="progress-bar" role="progressbar" aria-valuenow="88" aria-valuemin="0" aria-valuemax="100" style="width: 65%;">SEO 65%</div>
								</div>
							</li>
						</ul>
					</div>
				</div> -->
			</div> <!-- /#services -->

			<div id="portfolio" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>Selected Publications</h2>
						</div> <!-- /.section-title -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
				<div>
				A few selected publications are listed for each research direction. See <a href="https://scholar.google.com/citations?user=dbBKbXoAAAAJ&hl=zh-CN">Google Scholar</a>
				for a full list of publications.<br/>
			    </div><br/>
                <div class="layout">
                    <input name="nav" type="radio" class="rep-radio" id="rep" />
					<div class="page rep-page">
<div class="page-contents">
						<h1>Trusted Knowledge Exploration</h1>
                        <!--TODO: modify here-->
						<!-- <p>I innovate in representation learning techniques and embedding geometry for embedding for data with 
							different characteristics (hierarchical, heterogeneous etc.). </p>  -->
						
						<div class="row align-items-end">
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/Pinocchios.jpg" alt="HIE">
										<div class="overlay-p">
											<a href="images/papers/Pinocchios.jpg" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2310.05177">
										Do Large Language Models Know about Facts?</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/unforget.png" alt="HGTM">
										<div class="overlay-p">
											<a href="images/papers/unforget.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/pdf/2307.16230.pdf">
										An Unforgeable Publicly Verifiable Watermark for Large Language Models</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
                            <div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/semantic.png" alt="HGTM">
										<div class="overlay-p">
											<a href="images/papers/semantic.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/pdf/2307.16230.pdf">
										A Semantic Invariant Robust Watermark for Large Language Models</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
						</div> <!-- /.row -->
						<div class="row align-items-end paper-description">
							<div class="col-md-4 align-block">
								<p class="venue">ICLR 2024(Spotlight)</p>
								<p><strong>Xuming Hu</strong>, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo</p>
                                Pinocchio is a benchmark assessing large language models' (LLMs) factual knowledge across diverse domains and languages, through 20K questions. It reveals LLMs' limitations in handling factual information and spurious correlations, emphasizing challenges in achieving trustworthy artificial intelligence. 
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">ICLR 2024</p>
                                <p>Aiwei Liu, Leyi Pan, <strong>Xuming Hu</strong>, Shu’ang Li, Lijie Wen, Irwin King, Philip S. Yu</p>
                                Recent advancements in text watermarking for LLMs aim to mitigate issues like fake news and copyright infringement. Traditional watermark detection methods, reliant on a secret key, are vulnerable to security risks. To overcome this, a new unforgeable watermark algorithm has been developed, employing separate neural networks for watermark generation and detection, while sharing token embedding parameters for efficiency. 
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">ICLR 2024</p>
                                <p>Aiwei Liu, Leyi Pan, <strong>Xuming Hu</strong>, Shiao Meng, Lijie Wen</p>
                                To address the security and counterfeiting issues in current text watermarking for LLMs,  we employs distinct neural networks for watermark generation and detection, sharing token embedding parameters for efficiency. This approach ensures high detection accuracy and complicates forgery attempts, offering enhanced security and computational efficiency with fewer parameters.
							</div>
						</div>
						<hr style="height:1px;border-width:0;color:rgb(0, 17, 65);background-color:gray">

						<div class="row align-items-end">
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/Faithfully_FC.png" alt="GraphRNN">
										<div class="overlay-p">
											<a href="images/papers/Faithfully_FC.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2305.03507">
										Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
                            <div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/MR2.png" alt="HGTM">
										<div class="overlay-p">
											<a href="images/papers/MR2.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://dl.acm.org/doi/10.1145/3539618.3591896">
										MR2: A Benchmark for Multimodal Retrieval-Augmented Rumor Detection in Social Media</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
                            <div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/chef.png" alt="HGTM">
										<div class="overlay-p">
											<a href="images/papers/chef.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="http://arxiv.org/abs/2206.11863">
										CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
						</div> <!-- /.row -->
						<div class="row align-items-end paper-description">
							<div class="col-md-4 align-block">
								<p class="venue">SIGIR 2023</p>
                                <p><strong>Xuming Hu</strong>, Zhaochen Hong, Zhijiang Guo, Lijie Wen, Philip S. Yu</p>
                                ReRead is a fact verification model designed to enhance the accuracy of real-world fact verification tasks. It retrieves evidence from source documents, focusing on obtaining evidence that is both faithful (reflecting the model's decision-making process) and plausible (convincing to humans).
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">SIGIR 2023</p>
                                <p><strong>Xuming Hu</strong>, Zhijiang Guo, Junzhe Chen, Lijie Wen, Philip S. Yu</p>
								MR2 is a multimodal, multilingual dataset for rumor detection, addressing the evolving nature of misinformation on social media, which increasingly intertwines text and images. It offers a platform for developing advanced rumor detection systems capable of retrieving and reasoning over internet-sourced evidence from both text and image modalities. This dataset provides a challenging testbed for evaluating such systems.
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">NAACL 2022</p>
                                <p><strong>Xuming Hu</strong>, Zhijiang Guo, Guanyu Wu, Aiwei Liu, Lijie Wen, Philip S. Yu</p>
                                CHEF is the first Chinese Evidence-based Fact-checking dataset, featuring 10K real-world claims across various domains like politics and public health, with annotated evidence from the Internet. It aims to address the scarcity of non-English tools in automated fact-checking, particularly for Chinese.
							</div>
						</div>
						<hr style="height:1px;border-width:0;color:rgb(0, 17, 65);background-color:gray">
					</div>
					</div>
					<label class="nav" for="rep">
					  <span>
						<i class="fa-solid fa-chart-scatter-bubble fa-2x"></i>
						Trusted Knowledge Exploration
					  </span>
					</label>
                    
					<input name="nav" type="radio" class="gnn-radio" id="gnn" checked="checked" />
					<div class="row page gnn-page">
					  <div class="page-contents">
						<h1>Structured Knowledge Extraction</h1>
						<!-- TODO: modify here -->
                        <!-- <p>
						I focus on advancing graph neural network (GNN) architectures and improving the 
						expressiveness, scalability, interpretability and robustness of GNNs.
						More recently, I focus on pre-trained, large-scale foundation models for graph-structured data.
						</p>  -->
						<div class="row align-items-end" display="flex"> <!-- Row 1 -->
							<div class="col-md-4" display="flex">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/WebSelfORE.png" alt="MolGroup">
										<div class="overlay-p">
											<a href="images/papers/WebSelfORE.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://www.computer.org/csdl/journal/tk/5555/01/10255305/1QzyilOWR44">
										Reading Broadly to Open Your Mind: Improving Open Relation Extraction with Search Documents under Self-supervisions</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4" display="flex">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/MRE.jpg" alt="DeSCo">
										<div class="overlay-p">
											<a href="images/papers/MRE.jpg" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2305.16166">
										Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4" display="flex">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/promu.png" alt="DeSCo">
										<div class="overlay-p">
											<a href="images/papers/promu.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2310.16822">
										Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity and Relation Extraction</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
						</div> <!-- /.row -->
						<div class="row align-items-end paper-description">
							<div class="col-md-4 align-block">
								<p class="venue">IEEE TKDE 2024</p>
                                <p><strong>Xuming Hu</strong> , Zhaochen Hong, Chenwei Zhang, Aiwei Liu, Shiao Meng, Lijie Wen, Irwin King, Philip S. Yu                                </p>
                                Web-SelfORE is a self-supervised framework for open-domain relation extraction, utilizing a pretrained language model to analyze web documents and extract relational features. It enhances relation classification through adaptive clustering and self-supervised signals, showing superior performance on four public datasets compared to existing baselines.
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">ACL 2023</p>
                                <p><strong>Xuming Hu</strong>, Zhijiang Guo, Zhiyang Teng, Irwin King, Philip S. Yu
                                </p>
                                This research enhances multimodal relation extraction (MRE) by retrieving both textual and visual evidence from object, sentence, and image levels. A novel approach is developed to synthesize information across these levels for improved reasoning between modalities.
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">ACM MM 2023</p>
                                <p><strong>Xuming Hu</strong>, Junzhe Chen, Aiwei Liu, Shiao Meng, Lijie Wen, Philip S. Yu
                                </p>
                                PROMU is a novel approach to multimodal entity and relation extraction from text, leveraging unlabeled image-caption pairs for pre-training. It proposes unique objectives for aligning entities and relations with objects in images using soft pseudo-labels, enhancing extraction capabilities. 
							</div>
						</div><hr style="height:1px;border-width:0;color:gray;background-color:gray">
						<div class="row align-items-end"> <!-- Row 2 -->
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/Rationally_FC.png" alt="GNNExplainer">
										<div class="overlay-p">
											<a href="images/papers/Rationally_FC.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2305.03503">
										Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/SelfLRE.jpg" alt="NeuralExe">
										<div class="overlay-p">
											<a href="images/papers/SelfLRE.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://dl.acm.org/doi/pdf/10.1145/3539618.3592058">
										SelfLRE: Self-refining Representation Learning for Low-resource Relation Extraction</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/Scene.jpg" alt="IDGNN">
										<div class="overlay-p">
											<a href="images/papers/Scene.jpg" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2209.09093">Scene Graph Modification as Incremental Structure Expanding
                                    </a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
						</div> <!-- /.row -->
						<div class="row align-items-end paper-description">
							<div class="col-md-4 align-block">
								<p class="venue">SIGIR 2023</p>	
                                <p><strong>Xuming Hu</strong>, Zhaochen Hong, Chenwei Zhang, Irwin King, Philip S. Yu
                                </p>
                                RE2 is a rationale extraction framework for enhancing relation extraction by identifying relevant content and filtering out noise in sentences. It applies continuity and sparsity principles with an optimizable binary mask for token selection, ensuring semantic coherence. Demonstrated to surpass baselines in experiments on four datasets, RE2 effectively adjusts rationales in relation to given labels.
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">SIGIR 2023</p>
                                <p><strong>Xuming Hu</strong>, Junzhe Chen, Shiao Meng, Lijie Wen, Philip S. Yu
                                </p>
                                SelfLRE is a novel low-resource relation extraction architecture, blending self-training and self-ensembling learning to enhance task-specific representations on unlabeled data.
                            </div>
							<div class="col-md-4 align-block">
								<p class="venue">COLING 2022</p>	
                                <p><strong>Xuming Hu</strong>, Zhijiang Guo, Yu Fu, Lijie Wen, Philip S. Yu
                                </p>
                                ISE is a scene graph modification method that incrementally expands existing graphs based on natural language queries, maintaining unaltered structures. It efficiently iterates between node and edge predictions, showing significant improvements over previous models.
							</div>
						</div><hr style="height:1px;border-width:0;color:gray;background-color:gray">
						<div class="row">
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/HiURE.jpg" alt="GraphSAGE">
										<div class="overlay-p">
											<a href="images/papers/HiURE.jpg" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2205.02225">HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/GradLRE.jpg" alt="GraphRNN">
										<div class="overlay-p">
											<a href="images/papers/GradLRE.jpg" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2109.06415">Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction
                                    </a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/MetaSRE.jpg" alt="GNNExplainer">
										<div class="overlay-p">
											<a href="images/papers/MetaSRE.jpg" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2010.16410">Semi-supervised Relation Extraction via Incremental Meta Self-Training</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
						  </div> <!-- /.row -->
						  <div class="row align-items-end paper-description">
							<div class="col-md-4 align-block">
								<p class="venue">NAACL 2022</p>
                                <p>Shuliang Liu*, <strong>Xuming Hu*</strong>, Chenwei Zhang, Shu’ang Li, Lijie Wen, Philip S. Yu
                                </p>
								HiURE is a novel contrastive learning framework for unsupervised relation extraction, overcoming common issues in existing methods. It employs cross hierarchy attention to derive hierarchical signals from relational features and uses exemplar-wise contrastive learning for optimizing sentence relation representation. 
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">EMNLP 2021</p>
                                <p><strong>Xuming Hu</strong>, Chenwei Zhang, Yawen Yang, Xiaohe Li, Li Lin, Lijie Wen, Philip S. Yu
                                </p>
                                GradLRE is a method developed for low-resource relation extraction (LRE), addressing the challenges of pseudo label generation and feedback loops in learning paradigms. It employs a Gradient Imitation Reinforcement Learning approach to align pseudo label data with the gradient descent of labeled data.
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">EMNLP 2021 (Findings)</p>
                                <p><strong>Xuming Hu</strong>, Chenwei Zhang, Fukun Ma, Chenyao Liu, Lijie Wen, Philip S. Yu
                                </p>
                                MetaSRE is a semi-supervised relation extraction method designed to minimize reliance on large-scale annotations. It features a Relation Label Generation Network that evaluates pseudo labels through meta-learning, based on the performance of a Relation Classification Network. 
							</div>
						  </div>
						<!-- <hr style="height:1px;border-width:0;color:gray;background-color:gray"> -->
                        <div class="row">
							<div class="col-md-4">
								<div class="portfolio-item">
									<div class="portfolio-thumb">
										<img src="images/papers/SelfORE.jpg" alt="GraphSAGE">
										<div class="overlay-p">
											<a href="images/papers/SelfORE.jpg" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://arxiv.org/abs/2004.02438">SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction
                                    </a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
						  </div> <!-- /.row -->
                          <div class="row align-items-end paper-description">
							<div class="col-md-4 align-block">
								<p class="venue">EMNLP 2020 (Oral)</p>
                                <p><strong>Xuming Hu</strong>, Chenwei Zhang, Yusong Xu, Lijie Wen, Philip S. Yu</p>
                                SelfORE is a self-supervised framework for open relation extraction, utilizing a pretrained language model for adaptive clustering and bootstrapping self-supervised signals in relation classification.
							</div>
                            <div class="col-md-4 align-block">
								<p class="venue"></p>
							</div>
                            <div class="col-md-4 align-block">
								<p class="venue"></p>
							</div>
						  </div>
					  </div>
					  
					</div>
					<label class="nav" for="gnn">
					  <span>
						<i class="fa-solid fa-circle-nodes fa-2x"></i>
						Structed Knowledge Extraction
					  </span>
					</label>
				  

				  
					<!-- =================== Applications ================= -->
					<input name="nav" type="radio" class="applications-radio" id="applications" />
					<div class="page applications-page" style="vertical-align: top;">
					  <div class="page-contents" >
						<h1>Multimodal Knowledge Learning</h1>
						<!-- <p>Natural phenomenon and world's knowledge can often be expressed with the language of graphs. In addition to the popular applications of graphs such as
							<strong>social networks, recommender systems, knowledge graphs, biological networks and molecules</strong>, I'm also interested in novel ways of incorporating relational reasoning to other
							fields of science and technology, such as <strong>physical simulations, natural language and industrial relational database predictions</strong>.
						</p> -->
						<!-- <p><a href="#">Get in touch</a></p> -->
						<div class="row align-items-end">
							<div class="col-md-4">
                                <div class="portfolio-thumb">
                                    <img src="images/papers/promu.png" alt="DeSCo">
                                    <div class="overlay-p">
                                        <a href="images/papers/promu.png" data-gal="prettyPhoto">
                                            <i class="fa fa-arrows-alt fa-3x"></i>
                                        </a>
                                    </div>
                                </div> <!-- /.portfolio-thumb -->
                                <h3 class="portfolio-title"><a href="https://arxiv.org/abs/2310.16822">
                                    Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity and Relation Extraction</a></h3>
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4">
								<div class="portfolio-thumb">
                                    <img src="images/papers/MRE.jpg" alt="DeSCo">
                                    <div class="overlay-p">
                                        <a href="images/papers/MRE.jpg" data-gal="prettyPhoto">
                                            <i class="fa fa-arrows-alt fa-3x"></i>
                                        </a>
                                    </div>
                                </div> <!-- /.portfolio-thumb -->
                                <h3 class="portfolio-title"><a href="https://arxiv.org/abs/2305.16166">
                                    Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis</a></h3>
							</div> <!-- /.col-md-4 -->
							<div class="col-md-4">
								<div class="portfolio-item">
                                    <div class="portfolio-thumb">
										<img src="images/papers/MR2.png" alt="HGTM">
										<div class="overlay-p">
											<a href="images/papers/MR2.png" data-gal="prettyPhoto">
												<i class="fa fa-arrows-alt fa-3x"></i>
											</a>
										</div>
									</div> <!-- /.portfolio-thumb -->
									<h3 class="portfolio-title"><a href="https://dl.acm.org/doi/10.1145/3539618.3591896">
										MR2: A Benchmark for Multimodal Retrieval-Augmented Rumor Detection in Social Media</a></h3>
								</div> <!-- /.portfolio-item -->
							</div> <!-- /.col-md-4 -->
						</div> <!-- /.row -->
						<div class="row align-items-end paper-description">
							<div class="col-md-4 align-block">
								<p class="venue">MM 2023</p>
                                <p><strong>Xuming Hu</strong>, Junzhe Chen, Aiwei Liu, Shiao Meng, Lijie Wen, Philip S. Yu
                                </p>
								PROMU is a novel approach to multimodal entity and relation extraction from text, leveraging unlabeled image-caption pairs for pre-training. It proposes unique objectives for aligning entities and relations with objects in images using soft pseudo-labels, enhancing extraction capabilities. 
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">ACL 2023</p>
                                <p><strong>Xuming Hu</strong>, Zhaochen Hong, Zhijiang Guo, Lijie Wen, Philip S. Yu
                                </p>
								This research enhances multimodal relation extraction (MRE) by retrieving both textual and visual evidence from object, sentence, and image levels. A novel approach is developed to synthesize information across these levels for improved reasoning between modalities.
							</div>
							<div class="col-md-4 align-block">
								<p class="venue">SIGIR 2023</p>
                                <p><strong>Xuming Hu</strong>, Chenwei Zhang, Irwin King, Philip S. Yu
                                </p>
								MR2 is a multimodal, multilingual dataset for rumor detection, addressing the evolving nature of misinformation on social media, which increasingly intertwines text and images. It offers a platform for developing advanced rumor detection systems capable of retrieving and reasoning over internet-sourced evidence from both text and image modalities. This dataset provides a challenging testbed for evaluating such systems.
							</div>
						</div>
						<hr style="height:1px;border-width:0;color:rgb(0, 17, 65);background-color:gray">
					  </div>
					</div>
					<label class="nav" for="applications">
					  <span>
						<i class="fa-solid fa-user-gear fa-2x"></i>
						Multimodal Knowledge Learning
					  </span>
					  
					</label>
				</div>
				
				<!-- <div class="row">
					<div class="col-md-12">
						<div class="load-more">
							<a href="#portfolio" class="largeButton portfolioBgColor">Load More</a>
						</div>
					</div>
				</div> -->
			</div> <!-- /#portfolio -->

			<div id="student" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>To Prospective Students</h2>
						</div> <!-- /.section-title -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
				<div class="row">
					<div class="col-md-12">
						<div class="map-holder">
							For prospective students, I appreciate reading the following before reaching out to me through email.
							To make it easier for me to identify the applications, use "PhD (or Research Assistant , Visiting Student) Application" as your title.
							
							<h3>PhDs</h3>
							When reaching out to me, in addition to your CV, it would be best to demonstrate the following in your email.
							<ul>
								<li>Applicants do <strong>not</strong> need to have a degree in computer science, but they should possess good coding skills and a basic understanding of natural language processing. The research direction of the applicants does not need to align with mine, as long as they have sufficient interest in large language models.</li>
								<li>Prospective students are encouraged to visit our laboratory in advance to gain a better understanding of how our lab operates. This will facilitate a more informed decision based on mutual selection principles. </li>
								<li>Students who have only one publication as a (co-)first author, demonstrating their ability to develop ideas, implement, analyze, and write papers, are often considered more favorably than those who have participated in numerous publications without leading these projects.</li>
							</ul>
							<h4 style="color:blue;">Note:</h4> 
							I am aware that many students without a first-author publication record are interested in applying to our lab. We welcome these students to participate in our <strong>publication-oriented projects</strong>  or lead one <strong>influential open-source project</strong>. We are looking for self-motivated students. I will guide you in advancing these projects, and strong performance will significantly enhance your chances of a successful application.
							<!-- <h4 >Scholarship:</h4> 
                            Admitted Ph.D. students will receive a scholarship of ¥15k/month. -->
							<!-- <h4 style="color:red;font-weight: bold;">Note (special focus):</h4>
							<ul>
								<li>If you are interested in applying for PhD program focusing on AI for neuroscience, explicitly mention it in the email when reaching out to me.
									I encourage you to check out <a href="https://wti.yale.edu/research/neurocomputation">WTI (computational track)</a>, which I'm a part of.
								</li>
								<li> If you are interested in AI for computational biology, check out <a href="https://cbb.yale.edu/">Yale CBB program</a>, which I'm also a part of.
								</li>
							</ul>
							<h3>Postdocs</h3>
							Postdoc candidates are encouraged to reach out to me as well.  
							
							<ul>
								<li>Successful candidates usually have 3 or more 
									solid and impactful publications
									in an area, and have a coherent and unified thesis on a specific topic, encompassing a number of works.</li>
								<li>Similar to evaluating PhD applicants, I value paper quality over quantity. </li>
								<li>Prior experiences in leading a large-scope project will be appreciated.</li>
								<li>The candidates are required to have extensive research experiences in either foundation models, graph learning, trustworthy deep learning or relational reasoning.</li>
							</ul>

							After passing preliminary screening, The candidate will be asked to give a research talk (remote or in-person) to the group and talk to lab members, before
							making the decision. -->

							<h3>Research Assistants/Visiting Students</h3>
							I welcome research assistants, visiting students, and interns at all levels. Students are required to demonstrate a strong interest and good background knowledge in large language models. While prior research experience is encouraged, it is not mandatory. 
                            All positions for research assistants, visiting students, and internships can be remote. Research assistant positions will be compensated according to the applicant's background.
						</div> <!-- /.map-holder -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
			</div> <!-- /#Acknowledgement -->

			<!-- <div id="ack" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>Acknowledgement</h2>
						</div> 
					</div> 
				</div>  -->
				<!-- <div class="row">
					<div class="col-md-12">
						<div class="map-holder">
							My work would not have been possible without the support from my family, friends, students and awesome collaborators!
							Check out some of my collaborators:
							<a href="https://cs.stanford.edu/~jure/">Jure Leskovec</a>, 
							<a href="https://cs.stanford.edu/~chrismre/">Christopher Ré</a>,
							<a href="https://www.cst.cam.ac.uk/people/pl219">Pietro Liò</a>
							<a href="https://cs.stanford.edu/~jiaxuan/">Jiaxuan You</a>,
							<a href="https://scholar.harvard.edu/marinka">Marinka Zitnik</a>,
							<a href="https://www.cs.mcgill.ca/~wlh/">William Hamilton</a>,
							<a href="https://shanzhenren.github.io/">Xiang Ren</a>,
							<a href="https://scholar.google.com/citations?user=l_IWUOAAAAAJ">Bowen Liu</a>
							<a href="https://scholar.google.com/citations?user=nQ7Ij30AAAAJ">Peter Battaglia</a>,
							<a href="https://petar-v.com/">Petar Veličković</a>
							<a href="https://ines-chami.github.io/">Ines Chami</a>
							<a href="https://hanjun-dai.github.io/">Hanjun Dai</a>
							<a href="https://rusty1s.github.io/#/">Matthias Fey</a>
							and many more...

							<h3>Organizations</h3>
							Aside from university collaborations, 
							I also collaborated with many industrial companies and non-profit organizations including 
							<a href="https://www.pinterest.com/">Pinterest</a>, <a href="https://ai.facebook.com/research/">Facebook AI Research</a>,
							<a href="https://www.siemens.com/">Siemens</a>, <a href="https://www.deepmind.com/">DeepMind</a>, <a href="https://www.amazon.com/">Amazon</a>, 
							<a href="https://www6.slac.stanford.edu/">SLAC National Accelerator Library</a>,
							<a href="https://www.aramco.com/">Saudi Aramco</a>
							and more.
						</div> 
					</div> 
				</div> 
			</div> -->

			<div id="contact" class="section-content">
				<div class="row">
					<div class="col-md-12">
						<div class="section-title">
							<h2>Contact</h2>
						</div> <!-- /.section-title -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
				<div class="row">
					<div class="col-md-12">
						<!-- <div class="map-holder"> -->
							<div class="col-md-6" style="width: 100%;">
							<h3>Location</h3>
							I'm currently located at No.1 Du Xue Rd, Nansha District, Guangzhou.</br>
							<h3>Email</h3>
							You could reach me via email. 
							<a class="hiddenMail" data-email="xuminghu97@gmail.com">
								Show Email
							</a></br>
							I will try my best to respond if the schedule permits, unless I'm overwhelmed by emails.
						    </div>
							<!-- <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d1498.3289272415666!2d-72.9234615826982!3d41.31630624828283!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89e7d9b6cd624945%3A0xae34a2c4b4d30427!2sYale%20University!5e0!3m2!1sen!2sus!4v1657785511056!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe> -->
							<!-- <div class="google-map-canvas" id="map-canvas">
                    		</div>   -->
						<!--</div>  /.map-holder -->
					</div> <!-- /.col-md-12 -->
				</div> <!-- /.row -->
				<!-- <div class="row contact-form">
					<div class="col-md-4">
						<label for="name-id" class="required">Name:</label>
						<input name="name-id" type="text" id="name-id" maxlength="40">
					</div>
					<div class="col-md-4">
						<label for="email-id" class="required">Email:</label>
						<input name="email-id" type="text" id="email-id" maxlength="40">
					</div> 
					<div class="col-md-4">
						<label for="subject-id">Subject:</label>
						<input name="subject-id" type="text" id="subject-id" maxlength="60">
					</div>
					<div class="col-md-12">
						<label for="message-id" class="required">Message:</label>
						<textarea name="message-id" id="message-id" rows="6"></textarea>
					</div> 
					<div class="col-md-12">
						<div class="submit-btn">
							<a href="#" class="largeButton contactBgColor">Send Message</a>
						</div> 
					</div>
				</div> -->
			</div> <!-- /#contact -->

		</div> <!-- /.container-fluid -->

		<div class="site-footer">
			<div class="first-footer">
				<div class="container-fluid">
					<div class="row">
						<div class="col-md-12">
							<div class="social-footer" style="height: 100px; overflow: hidden;">
								<!-- <ul> -->
                                    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=167&t=tt&d=NS7Xb2ntxSF50vm4bXUwOkBU55Ea3cDDlHpYLt4pWz4'></script>
								<!-- </ul> -->
							</div> <!-- /.social-footer -->
						</div> <!-- /.col-md-12 -->
					</div> <!-- /.row -->
				</div> <!-- /.container-fluid -->
			</div> <!-- /.first-footer -->
			<div class="bottom-footer">
				<div class="container-fluid">
					<div class="row">
						<div class="col-md-6">
							<p class="copyright">Web template borrow from <a href="https://www.cs.yale.edu/homes/ying-rex/">Rex (Zhitao) Ying</a>
                            </p>
						</div> <!-- /.col-md-6 -->
						<div class="col-md-6 credits">
							<p><!-- Design: <a rel="nofollow" href="http://www.templatemo.com/tm-394-sonic" target="_parent">Sonic</a> --></p>
						</div> <!-- /.col-md-6 -->
					</div> <!-- /.row -->
				</div> <!-- /.container-fluid -->
			</div> <!-- /.bottom-footer -->
		</div> <!-- /.site-footer -->

	</div> <!-- /#main-content -->

	<!-- JavaScripts -->
	<script src="js/jquery-1.10.2.min.js"></script>
	<script src="js/jquery.singlePageNav.js"></script>
	<script src="js/jquery.flexslider.js"></script>
	<script src="js/jquery.prettyPhoto.js"></script>
	<script src="js/custom.js"></script>
	<script>
		$(document).ready(function(){
			$("a[data-gal^='prettyPhoto']").prettyPhoto({hook: 'data-gal'});

			$('.expand-list2 ul').hide();
			$('.expand-list ul').hide();

			$('.expand-list > li').click(function() {
				$(this).parent().find('ul').slideToggle();
			});

			$('.expand-list2 > li').click(function() {
				$(this).parent().find('ul').slideToggle();
			});

		});

        function initialize() {
          var mapOptions = {
            zoom: 13,
            center: new google.maps.LatLng(40.7809919,-73.9665273)
          };

          var map = new google.maps.Map(document.getElementById('map-canvas'),
              mapOptions);
        }

        function loadScript() {
          var script = document.createElement('script');
          script.type = 'text/javascript';
          script.src = 'https://maps.googleapis.com/maps/api/js?v=3.exp&sensor=false&' +
              'callback=initialize';
          document.body.appendChild(script);
        }

		$(window).load(function () {
			$('.hiddenMail').on('click',function(event)
			{
				event.preventDefault();
				$(this).off("click");
				var email = $(this).attr("data-email").replace(/AT/,'@').replace(/DOT/,'.');
				$(this).removeClass("hiddenMail");
				$(this).html(email);
				$(this).attr("href","mailto:"+email);
			});
		});

        window.onload = loadScript;
    </script>
<!-- templatemo 394 sonic -->
</body>
</html>
